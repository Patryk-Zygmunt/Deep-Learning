{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# korekta błędu w Keras po zmianie w bibliotece numpy\n",
    "import numpy as np\n",
    "np_load_old = np.load\n",
    "np.load = lambda *a, **k: np_load_old(*a, allow_pickle=True, **k)\n",
    "\n",
    "# wyłączenie ostrzeżeń\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '-1'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.2.4'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.2 Trenowanie konwolucyjnej sieci neuronowej na małym zbiorze danych\n",
    "\n",
    "\n",
    "Konieczność trenowania modelu klasyfikacji obrazów na bardzo małej ilości danych jest często spotykaną sytuacją podczas prywatnej pracy nad problemami analizy obrazu. „Mała liczba” próbek może oznaczać różną liczbę — od kilkuset do kilkudziesięciu tysięcy obrazów. W tym podrozdziale zajmiemy się praktycznym przykładem klasyfikacji zdjęć przedstawiających psy i koty. Nasz zbiór będzie składał się z 4000 obrazów (2000 z nich będzie przedstawiać koty, a pozostałe 2000 psy). Podczas testowania będziemy korzystać z 2000 zdjęć, 1000 przyda nam się do walidacji, a kolejny 1000 zostanie użyty do testowania.\n",
    "\n",
    "W tym podrozdziale zajmiemy się jedną strategią rozwiązywania tego problemu — będziemy trenować nowy model od podstaw, korzystając przy tym tylko z dostępnych danych. Zaczniemy od naiwnego trenowania konwolucyjnej sieci neuronowej na 2000 próbek bez stosowania mechanizmu regularyzacji. Utworzymy w ten sposób punkt odniesienia do dalszej pracy — nasz klasyfikator uzyska dokładność na poziomie 71%. Naszym głównym problemem będzie nadmierne dopasowanie modelu do danych treningowych. Wprowadzimy technikę augmentacji danych, która pozwala na zmniejszenie skutków zbytniego dopasowania modelu w przypadku problemów dotyczących przetwarzania obrazu. Technika ta pozwoli zwiększyć dokładność modelu do 82%.\n",
    "\n",
    "W dalszej części tego rozdziału opiszę dwie kolejne techniki przydatne podczas stosowania uczenia głębokiego na małych zbiorach danych: ekstrakcję cech przy uprzednio trenowanej sieci (rozwiązanie to pozwala na uzyskanie dokładności sięgającej 90–96%) i dostrajanie uprzednio trenowanej sieci (technika ta umożliwia uzyskanie dokładności na poziomie 97%). Te trzy techniki (trenowanie małego modelu od podstaw, ekstrakcja cech przy uprzednio trenowanej sieci i dostrajanie uprzednio trenowanej sieci) pozwolą Ci na późniejszą samodzielną pracę nad problemami klasyfikacji obrazów przy dysponowaniu małą ilością danych.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stosowanie uczenia głębokiego w problemach małych zbiorów danych\n",
    "\n",
    "Niektórzy twierdzą, że uczenie głębokie działa tylko wtedy, gdy możliwe jest uzyskanie dostępu do dużej ilości danych. Stwierdzenie to jest częściowo prawdziwe: główną cechą uczenia głębokiego jest to, że algorytmy tego uczenia mogą samodzielnie wybrać przydatne cechy z treningowego zbioru danych, ale wymagają do tego licznego treningowego zbioru danych. Dotyczy to szczególnie pracy z próbkami o bardzo dużej liczbie wymiarów (przykładem takich próbek są obrazy).\n",
    "\n",
    "Pojęcie licznego treningowego zbioru danych jest względne. Liczba danych potrzebnych do wytrenowania sieci zależy np. od jej rozmiaru i głębokości. Konwolucyjnej sieci neuronowej nie można wytrenować w celu rozwiązania skomplikowanego problemu na zaledwie kilkudziesięciu przykładach, ale zbiór kilkuset przykładów może okazać się wystarczający, jeżeli model będzie mały i poddany regularyzacji, a zadanie będzie proste. Konwolucyjne sieci neuronowe uczą się lokalnych cech niewrażliwych na przesunięcie, a więc charakteryzują się dużą wydajnością analizy danych w przypadku problemów percepcyjnych. Trenowanie konwolucyjnej sieci neuronowej od podstaw na bardzo małym zbiorze obrazów może dać całkiem sensowne efekty pomimo relatywnego braku danych (bez potrzeby przeprowadzania specjalnej inżynierii cech). Przekonasz się o tym podczas lektury tego podrozdziału.\n",
    "\n",
    "Ponadto modele uczenia głębokiego mają naturę umożliwiającą stosowanie ich w wielu celach — model klasyfikacji obrazu lub dokonujący konwersji mowy na tekst pisany, który to model został wytrenowany na dużym zbiorze danych, może zostać użyty w celu rozwiązania innego problemu przy niewielkiej ilości zmian. Szczególnie w przypadku przetwarzania obrazu wiele uprzednio wytrenowanych modeli (zwykle modele te trenuje się na zbiorze danych ImageNet) może zostać pobranych z internetu i zastosowanych podczas pracy z małą ilością danych — zabieg ten pozwala na uzyskanie doskonałych wyników. Przykład zastosowania tej techniki przedstawię w dalszej części tego rozdziału. Zacznijmy pracę nad naszym modelem od wczytania danych."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pobieranie danych\n",
    "\n",
    "Będziemy korzystać ze zbioru danych „Dogs vs. Cats”, który nie jest dołączony do pakietu Keras. Został on udostępniony w serwisie Kaggle w ramach konkursu analizy obrazu pod koniec 2013 r. (wówczas sieci konwolucyjne nie były jeszcze popularne). Możesz go pobrać ze strony http://www.kaggle.com/c/dogs-vs-cats/data (musisz posiadać konto w serwisie Kaggle, ale jeżeli go jeszcze nie masz, to założenie go nie będzie stanowić żadnego problemu).\n",
    "\n",
    "Zdjęcia wchodzące w skład zbioru są kolorowymi obrazami JPEG o średniej rozdzielczości. Oto kilka przykładowych zdjęć ze zbioru, który będziemy za chwilę przetwarzać:\n",
    "\n",
    "\n",
    "![cats_vs_dogs_samples](https://s3.amazonaws.com/book.keras.io/img/ch5/cats_vs_dogs_samples.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oczywiście konkurs z 2013 r., z którego pochodzi ten zbiór danych, został wygrany przez osoby, które użyły konwolucyjnych sieci neuronowych. Najlepsze rozwiązania uzyskały dokładność na poziomie 95%. W tym przykładzie (w kolejnej sekcji) zbliżysz się do tej wartości pomimo tego, że Twój model będzie trenowany na próbce mniej niż 10% danych udostępnionych uczestnikom konkursu.\n",
    "\n",
    "Pełny zbiór danych zawiera 25 000 zdjęć psów i kotów (po 12 500 zdjęć należących do każdej z klas) i po skompresowaniu zajmuje 543 MB. Po pobraniu go i rozpakowaniu utworzymy nowy zbiór składający się z trzech podzbiorów: zbioru treningowego zawierającego po 1000 próbek każdej z klas, zbioru walidacyjnego zawierającego po 500 próbek każdej z klas i zbioru testowego zawierającego po 500 próbek każdej z klas.\n",
    "\n",
    "Oto kod, który wykonuje te operacje:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileExistsError",
     "evalue": "[WinError 183] Nie można utworzyć pliku, który już istnieje: 'Data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-c3de9b818f80>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Katalog, w którym umieszczone zostaną mniejsze zbiory danych.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mbase_dir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'Data'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# Katalogi podzbiorów (zbioru treningowego, walidacyjnego i testowego).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileExistsError\u001b[0m: [WinError 183] Nie można utworzyć pliku, który już istnieje: 'Data'"
     ]
    }
   ],
   "source": [
    "# Ścieżka katalogu, do którego rozpakowano oryginalny zbiór danych.\n",
    "original_dataset_dir = 'DogsCats'\n",
    "\n",
    "# Katalog, w którym umieszczone zostaną mniejsze zbiory danych.\n",
    "base_dir = 'Data'\n",
    "os.mkdir(base_dir)\n",
    "\n",
    "# Katalogi podzbiorów (zbioru treningowego, walidacyjnego i testowego).\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "os.mkdir(train_dir)\n",
    "validation_dir = os.path.join(base_dir, 'validation')\n",
    "os.mkdir(validation_dir)\n",
    "test_dir = os.path.join(base_dir, 'test')\n",
    "os.mkdir(test_dir)\n",
    "\n",
    "# Katalog z treningowym zbiorem zdjęć kotów.\n",
    "train_cats_dir = os.path.join(train_dir, 'cats')\n",
    "os.mkdir(train_cats_dir)\n",
    "\n",
    "# Katalog z treningowym zbiorem zdjęć psów.\n",
    "train_dogs_dir = os.path.join(train_dir, 'dogs')\n",
    "os.mkdir(train_dogs_dir)\n",
    "\n",
    "# Katalog z walidacyjnym zbiorem zdjęć kotów.\n",
    "validation_cats_dir = os.path.join(validation_dir, 'cats')\n",
    "os.mkdir(validation_cats_dir)\n",
    "\n",
    "# Katalog z walidacyjnym zbiorem zdjęć psów.\n",
    "validation_dogs_dir = os.path.join(validation_dir, 'dogs')\n",
    "os.mkdir(validation_dogs_dir)\n",
    "\n",
    "# Katalog z testowym zbiorem zdjęć kotów.\n",
    "test_cats_dir = os.path.join(test_dir, 'cats')\n",
    "os.mkdir(test_cats_dir)\n",
    "\n",
    "# Katalog z testowym zbiorem zdjęć psów.\n",
    "test_dogs_dir = os.path.join(test_dir, 'dogs')\n",
    "os.mkdir(test_dogs_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DogsCats\\\\cat.0.jpg'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Kopiuje 1000 pierwszych zdjęć kotów do folderu train_cats_dir.\n",
    "fnames = ['cat.{}.jpg'.format(i) for i in range(1000)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(train_cats_dir, fname)\n",
    "    shutil.copyfile(src, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kopiuje 500 kolejnych obrazów kotów do folderu validation_cats_dir.\n",
    "fnames = ['cat.{}.jpg'.format(i) for i in range(1000, 1500)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(validation_cats_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "    \n",
    "# Kopiuje 500 kolejnych obrazów kotów do folderu test_cats_dir.\n",
    "fnames = ['cat.{}.jpg'.format(i) for i in range(1500, 2000)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(test_cats_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "    \n",
    "# Kopiuje 1000 pierwszych zdjęć psów do folderu train_dogs_dir.\n",
    "fnames = ['dog.{}.jpg'.format(i) for i in range(1000)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(train_dogs_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "    \n",
    "# Kopiuje 500 kolejnych obrazów kotów do folderu validation_dogs_dir.\n",
    "fnames = ['dog.{}.jpg'.format(i) for i in range(1000, 1500)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(validation_dogs_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "    \n",
    "# Kopiuje 500 kolejnych obrazów kotów do folderu test_dogs_dir.\n",
    "fnames = ['dog.{}.jpg'.format(i) for i in range(1500, 2000)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(test_dogs_dir, fname)\n",
    "    shutil.copyfile(src, dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sprawdźmy, ile zdjęć mamy w poszczególnych podzbiorach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "liczba obrazów treningowych kotów: 1000\n"
     ]
    }
   ],
   "source": [
    "print('liczba obrazów treningowych kotów:', len(os.listdir(train_cats_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "liczba obrazów treningowych psów: 1000\n"
     ]
    }
   ],
   "source": [
    "print('liczba obrazów treningowych psów:', len(os.listdir(train_dogs_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "liczba obrazów walidacyjnych kotów: 500\n"
     ]
    }
   ],
   "source": [
    "print('liczba obrazów walidacyjnych kotów:', len(os.listdir(validation_cats_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "liczba obrazów walidacyjnych psów: 500\n"
     ]
    }
   ],
   "source": [
    "print('liczba obrazów walidacyjnych psów:', len(os.listdir(validation_dogs_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "liczba obrazów testowych kotów: 500\n"
     ]
    }
   ],
   "source": [
    "print('liczba obrazów testowych kotów:', len(os.listdir(test_cats_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "liczba obrazów testowych psów: 500\n"
     ]
    }
   ],
   "source": [
    "print('liczba obrazów testowych psów:', len(os.listdir(test_dogs_dir)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Uzyskaliśmy zbiór treningowy składający się z 2000 zdjęć, zbiór walidacyjny składający się z 1000 zdjęć i zbiór testowy również zawierający 1000 zdjęć. Każdy zbiór zawiera równą liczbę zdjęć każdej z klas — pracujemy z problemem wyważonej klasyfikacji binarnej, a więc dokładność klasyfikacji jest miarą sukcesu pracy modelu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Budowa sieci neuronowej\n",
    "\n",
    "W poprzednim przykładzie tworzyliśmy konwolucyjną sieć neuronową przetwarzającą zbiór danych MNIST, a więc tworzenie takiej sieci nie jest dla Ciebie niczym nowym. Ponownie zastosujemy strukturę tej sieci: nasza sieć będzie stosem naprzemiennych warstw Conv2D (z funkcja aktywacji relu) i MaxPooling2D.\n",
    "\n",
    "Tym razem pracujemy z większymi obrazami i bardziej złożonym problemem, a więc musimy dostosować do niego konstrukcję sieci — dodamy do niej jeszcze jedną fazę Conv2D + MaxPooling2D. Rozwiązanie to zmodyfikuje pojemność sieci i zredukuje rozmiar map cech tak, aby nie były one zbyt duże po osiągnięciu warstwy spłaszczania Flatten. Zaczynamy od map wejściowych o rozmiarach 150x150 (to wybrana przeze mnie dowolna wartość), a tuż przed warstwą Flatten kończymy na mapach o rozmiarze 7x7.\n",
    "\n",
    "\n",
    "Zauważ, że w tej sieci głębokość map cech wzrasta w sposób progresywny (od 32 do 128), a ich wymiary maleją (od 148148 do 77). Sytuacja taka ma miejsce w prawie wszystkich konwolucyjnych sieciach neuronowych.\n",
    "\n",
    "Próbujemy rozwiązać problem klasyfikacji binarnej, a więc na końcu sieci umieszczamy jedną jednostkę (warstwę Dense o rozmiarze równym 1) i funkcję aktywacji sigmoid. Jednostka ta będzie generować wartości prawdopodobieństwa tego, że analizowany obraz należy do jednej z klas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras import models\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu',\n",
    "                        input_shape=(150, 150, 3)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przyjrzyjmy się zmianom liczby wymiarów przyszłych map w kolejnych warstwach sieci:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 148, 148, 32)      896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 74, 74, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 72, 72, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 36, 36, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 34, 34, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 17, 17, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 15, 15, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               3211776   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 3,453,121\n",
      "Trainable params: 3,453,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na etapie kompilacji skorzystamy jak zwykle z optymalizatora RMSprop. Zakończyliśmy sieć pojedynczą jednostką sigmoid, a wiec skorzystamy z funkcji straty w postaci binarnej entropii krzyżowej (informacje na temat doboru funkcji straty do różnych sytuacji znajdziesz w tabeli 4.1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=1e-4),\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wstępna obróbka danych\n",
    "\n",
    "Przypominam, że dane przed przekazaniem do wejść sieci należy odpowiednio sformatować — przedstawić w formie tensorów wartości zmiennoprzecinkowych. Obecnie dane zapisane na dysku mają formę plików JPG, a w celu przystosowania ich do przetwarzania przez sieć należy:\n",
    "\n",
    "* Wczytać pliki obrazów.\n",
    "* Zdekodować format JPEG do siatki pikseli w formacie RGB.\n",
    "* Zapisać dane w formie tensorów liczb zmiennoprzecinkowych.\n",
    "* Przeskalować wartości pikseli z zakresu 0–255 do zakresu [0, 1], ponieważ sieci neuronowe lepiej pracują z małymi wartościami wejściowymi.\n",
    "\n",
    "Może się wydawać to dość pracochłonne, ale pakiet Keras jest wyposażony w narzędzia umożliwiające automatyczne wykonanie procesu konwersji. Pakiet Keras ma moduł keras.preprocessing.image zawierający narzędzia przeznaczone do przetwarzania obrazów. W module tym znajduje się klasa ImageDataGenerator, pozwalająca szybko skonfigurować generatory Pythona, które automatycznie zamienią obrazy zapisane na dysku w tensory przygotowane do skierowania do sieci neuronowej. Skorzystamy z tego gotowego rozwiązania:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n",
      "Found 1000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Przeskalowuje wszystkie obrazy o współczynnik 1/255.\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        # Katalog docelowy.\n",
    "        train_dir,\n",
    "        # Zmienia rozdzielczość wszystkich obrazów na 150x150.\n",
    "        target_size=(150, 150),\n",
    "        batch_size=20,\n",
    "        # Używamy funkcji binary_crossentropy w charakterze funkcji straty, a więc potrzebujemy binarnych etykiet.\n",
    "        class_mode='binary')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        validation_dir,\n",
    "        target_size=(150, 150),\n",
    "        batch_size=20,\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przyjrzyjmy się wartościom wygenerowanym przez jeden z generatorów: zwraca on wsad obrazów RGB o wymiarach 150150 (o kształcie (20, 150, 150, 3)) i binarne etykiety (kształt (20, )). W każdym wsadzie znajduje się 20 próbek. Zauważ, że generator zwraca wsady w nieskończoność (wykonuje nieskończoną pętlę, przetwarzając obrazy umieszczone w folderze docelowym). W związku z tym w pewnym momencie pętla musi zostać przerwana poleceniem break:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kształt danych wsadowych: (20, 150, 150, 3)\n",
      "kształt etykiet danych wsadowych: (20,)\n"
     ]
    }
   ],
   "source": [
    "for data_batch, labels_batch in train_generator:\n",
    "    print('kształt danych wsadowych:', data_batch.shape)\n",
    "    print('kształt etykiet danych wsadowych:', labels_batch.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dopasujmy model do danych przy użyciu generatora. W tym celu należy skorzystać z metody fit_generator — jest to odpowiednik metody fit stosowany w przypadku generatorów danych. Metoda ta oczekuje zdefiniowania w pierwszym argumencie generatora, który w nieskończoność będzie zwracał wsady danych wejściowych i ich etykiet. Dane są generowane w nieskończoność, a więc model Keras musi wiedzieć, ile próbek ma pobrać z generatora przed zakończeniem epoki. Służy do tego argument steps_per_epoch: po pobraniu liczby wsadów określanej przez wartość tego argumentu (tj. po wykonaniu odpowiedniej liczby kroków spadku gradientu) proces dopasowywania modelu przejdzie do kolejnej epoki. W naszym przypadku wsady składają się z 20 próbek, a więc musimy wygenerować 100 wsadów w celu wytrenowania modelu na 2000 próbek.\n",
    "\n",
    "Korzystając z metody fit_generator, możemy — podobnie jak w przypadku metody fit — przekazać argument validation_data. Argument ten może być generatorem danych, a także krotką tablic Numpy. W przypadku przekazania generatora w argumencie validation_data oczekuje się, że generator ten będzie zwracał wsady danych walidacyjnych w nieskończoność. W związku z tym należy zdefiniować wartość argumentu validation_steps określającą liczbę wsadów, która ma zostać pobrana z generatora danych walidacyjnych w celu przeprowadzenia walidacji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "100/100 [==============================] - 55s 547ms/step - loss: 0.6927 - acc: 0.5285 - val_loss: 0.6885 - val_acc: 0.5000\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 66s 655ms/step - loss: 0.6651 - acc: 0.6015 - val_loss: 0.6337 - val_acc: 0.6470\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 64s 642ms/step - loss: 0.6196 - acc: 0.6685 - val_loss: 0.6112 - val_acc: 0.6620\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 57s 574ms/step - loss: 0.5762 - acc: 0.7010 - val_loss: 0.6016 - val_acc: 0.6710\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 61s 607ms/step - loss: 0.5511 - acc: 0.7065 - val_loss: 0.6026 - val_acc: 0.6620\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 70s 695ms/step - loss: 0.5098 - acc: 0.7455 - val_loss: 0.6043 - val_acc: 0.6820\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 80s 799ms/step - loss: 0.4868 - acc: 0.7540 - val_loss: 0.6144 - val_acc: 0.6560\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 83s 825ms/step - loss: 0.4570 - acc: 0.7920 - val_loss: 0.5694 - val_acc: 0.6950\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 85s 848ms/step - loss: 0.4381 - acc: 0.7980 - val_loss: 0.5734 - val_acc: 0.7040\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 74s 742ms/step - loss: 0.4127 - acc: 0.8100 - val_loss: 0.5687 - val_acc: 0.7160\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 75s 750ms/step - loss: 0.3863 - acc: 0.8320 - val_loss: 0.5399 - val_acc: 0.7390\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 76s 759ms/step - loss: 0.3631 - acc: 0.8430 - val_loss: 0.5858 - val_acc: 0.7260\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 78s 780ms/step - loss: 0.3477 - acc: 0.8520 - val_loss: 0.5560 - val_acc: 0.7340\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 76s 764ms/step - loss: 0.3147 - acc: 0.8690 - val_loss: 0.5581 - val_acc: 0.7400\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 78s 778ms/step - loss: 0.2937 - acc: 0.8835 - val_loss: 0.5648 - val_acc: 0.7430\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 60s 598ms/step - loss: 0.2655 - acc: 0.8955 - val_loss: 0.5831 - val_acc: 0.7380\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 70s 698ms/step - loss: 0.2543 - acc: 0.8955 - val_loss: 0.6799 - val_acc: 0.7150\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 63s 631ms/step - loss: 0.2294 - acc: 0.9140 - val_loss: 0.5864 - val_acc: 0.7500\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 66s 659ms/step - loss: 0.2120 - acc: 0.9175 - val_loss: 0.5976 - val_acc: 0.7440\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 75s 753ms/step - loss: 0.1855 - acc: 0.9355 - val_loss: 0.6509 - val_acc: 0.7430\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 76s 756ms/step - loss: 0.1673 - acc: 0.9410 - val_loss: 0.7459 - val_acc: 0.7100\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 79s 786ms/step - loss: 0.1435 - acc: 0.9530 - val_loss: 0.6791 - val_acc: 0.7410\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 76s 759ms/step - loss: 0.1365 - acc: 0.9565 - val_loss: 0.6870 - val_acc: 0.7290\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 76s 764ms/step - loss: 0.1140 - acc: 0.9600 - val_loss: 0.7282 - val_acc: 0.7420\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 75s 753ms/step - loss: 0.0984 - acc: 0.9675 - val_loss: 0.7297 - val_acc: 0.7470\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 74s 743ms/step - loss: 0.0880 - acc: 0.9720 - val_loss: 0.8376 - val_acc: 0.7240\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 72s 716ms/step - loss: 0.0766 - acc: 0.9805 - val_loss: 0.8184 - val_acc: 0.7360\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 70s 696ms/step - loss: 0.0682 - acc: 0.9815 - val_loss: 0.8191 - val_acc: 0.7290\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 67s 670ms/step - loss: 0.0629 - acc: 0.9795 - val_loss: 0.8508 - val_acc: 0.7330\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 65s 655ms/step - loss: 0.0420 - acc: 0.9930 - val_loss: 0.8442 - val_acc: 0.7490\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(\n",
    "      train_generator,\n",
    "      steps_per_epoch=100,\n",
    "      epochs=30,\n",
    "      validation_data=validation_generator,\n",
    "      validation_steps=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Dobrą praktyką jest zapisywanie wszystkich wytrenowanych modeli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('cats_and_dogs_small_1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utwórzmy wykresy straty i dokładności pracy modelu podczas przetwarzania danych treningowych i walidacyjnych\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Dokladnosc trenowania')\n",
    "plt.plot(epochs, val_acc, 'b', label='Dokladnosc walidacji')\n",
    "plt.title('Dokladnosc trenowania i walidacji')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Strata trenowania')\n",
    "plt.plot(epochs, val_loss, 'b', label='Strata walidacji')\n",
    "plt.title('Strata trenowania i walidacji')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na wykresach tych wyraźnie widać nadmierne dopasowanie. Dokładność trenowania wzrasta liniowo wraz z upływem czasu aż do osiągnięcia wartości równej niemalże 100%, a dokładność walidacji nie przekracza poziomu 70 – 72%. Strata walidacji osiąga minimalną wartość po zaledwie pięciu epokach, a następnie stabilizuje się, a strata treningowa maleje liniowo aż do osiągnięcia wartości zbliżonych do 0.\n",
    "\n",
    "Dysponujemy względnie niewielką liczbą próbek treningowych (2000), a więc nadmierne dopasowanie będzie naszym głównym problemem. Znasz już kilka technik rozwiązywania tego problemu, takich jak porzucanie i rozkład wag (regularyzacja L2). Teraz poznasz nową technikę przeciwdziałania nadmiernemu dopasowaniu, która sprawdza się podczas analizy obrazu i jest używana w praktycznie wszystkich modelach uczenia głębokiego przetwarzających obrazy: augmentację danych."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stosowanie techniki augmentacji danych\n",
    "\n",
    "Nadmierne dopasowanie wynika ze zbyt małej liczby próbek, na których model może się uczyć. Model nie może w takiej sytuacji utworzyć uogólnień, które sprawdzą się podczas przetwarzania nowych danych. Gdybyśmy dysponowali nieskończenie wielkim zbiorem danych treningowych, to na model działałby każdy możliwy aspekt rozkładu danych — nigdy nie uległby przeuczeniu. Augmentacja danych to technika generowania większej liczby elementów treningowego zbioru danych poprzez augmentację próbek na drodze losowych przekształceń zwracających obrazy, które wyglądają wiarygodnie. Celem tego rozwiązania jest to, aby trenowany model nigdy nie zobaczył dwukrotnie tego samego zdjęcia. Dzięki temu model może zauważyć więcej aspektów przetwarzanych danych i utworzyć lepsze uogólnienia.\n",
    "\n",
    "W pakiecie Keras z techniki tej można skorzystać, konfigurując losowe przekształcenia obrazów wczytywanych przez instancję ImageDataGenerator. Zacznijmy od przeanalizowania przykładu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "      rotation_range=40,\n",
    "      width_shift_range=0.2,\n",
    "      height_shift_range=0.2,\n",
    "      shear_range=0.2,\n",
    "      zoom_range=0.2,\n",
    "      horizontal_flip=True,\n",
    "      fill_mode='nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To tylko kilka z dostępnych opcji (informacje na temat pozostałych znajdziesz w dokumentacji pakietu Keras). Przeanalizujmy zaprezentowany kod:\n",
    "\n",
    "* Wartość rotation_range określa stopnie (0–180) — zakres kątów, o które zostanie wykonany losowy obrót obrazów.\n",
    "* Zakresy width_shift i height_shift określają ułamki całkowitej szerokości i wysokości obrazów, zakresy te określają ramy, w obrębie których przeprowadza się losowe pionowe i poziome przekształcenia obrazów.\n",
    "* Parametr shear_range określa zakres losowego przycinania obrazu.\n",
    "* Parametr zoom_range określa zakres losowego przybliżania fragmentów obrazów.\n",
    "* Operacja horizontal_flip polega na losowym odbiciu połowy obrazu w płaszczyźnie poziomej — z przekształcenia tego warto korzystać wtedy, gdy nie ma założeń o horyzontalnej asymetrii obrazu (np. w przypadku prawdziwych zdjęć).\n",
    "* Tryb fill_mode jest strategią wypełniania nowo utworzonych pikseli, które mogą powstać w wyniku obrotu lub przesunięcia.\n",
    "\n",
    "Przyjrzyjmy się zmodyfikowanym obrazom:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-4c42885043c5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mfnames\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_cats_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_cats_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# Wybieramy obraz do zmodyfikowania.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "# Operacja importowania modułu zawierającego narzędzia przetwarzajace obrazy.\n",
    "from keras.preprocessing import image\n",
    "\n",
    "fnames = [os.path.join(train_cats_dir, fname) for fname in os.listdir(train_cats_dir)]\n",
    "\n",
    "# Wybieramy obraz do zmodyfikowania.\n",
    "img_path = fnames[3]\n",
    "\n",
    "# Wczytujemy obraz i zmieniamy jego rozdzielczość.\n",
    "img = image.load_img(img_path, target_size=(150, 150))\n",
    "\n",
    "# Zamieniamy obraz w tablicę Numpy o kształcie (150, 150, 3).\n",
    "x = image.img_to_array(img)\n",
    "\n",
    "# Zmieniamy kształt na (1, 150, 150, 3).\n",
    "x = x.reshape((1,) + x.shape)\n",
    "\n",
    "# Polecenie .flow() generuje wsady obrazów zmodyfikowanych w sposób losowy. \n",
    "# Pętla jest wykonywana w nieskończoność, a więc należy ją w pewnym momencie przerwać!\n",
    "i = 0\n",
    "for batch in datagen.flow(x, batch_size=1):\n",
    "    plt.figure(i)\n",
    "    imgplot = plt.imshow(image.array_to_img(batch[0]))\n",
    "    i += 1\n",
    "    if i % 4 == 0:\n",
    "        break\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jeżeli użyjemy tak skonfigurowanego mechanizmu modyfikującego obrazy, to nasza sieć nigdy nie będzie przetwarzać dwukrotnie tego samego obrazu, ale przetwarzane przez nią obrazy są wciąż bardzo podobne do siebie, ponieważ generujemy je na bazie małej liczby oryginalnych obrazów — nie możemy wygenerować nowych informacji, lecz tylko przedstawiać w nowej formie informacje, którymi dysponujemy. W związku z tym być może nie uda nam się zupełnie wyeliminować nadmiernego dopasowania. W związku z tym później dodamy do sieci warstwę Dropout (umieścimy ją bezpośrednio przed gęsto połączonym klasyfikatorem)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'models' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-333ecaeaaace>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m model.add(layers.Conv2D(32, (3, 3), activation='relu',\n\u001b[0;32m      3\u001b[0m                         input_shape=(150, 150, 3)))\n\u001b[0;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMaxPooling2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'models' is not defined"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu',\n",
    "                        input_shape=(150, 150, 3)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=1e-4),\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przeprowadźmy proces trenowania sieci przy użyciu technik augmentacji danych i odrzucania."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-3a75a53b7175>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m train_generator = train_datagen.flow_from_directory(\n\u001b[0;32m     14\u001b[0m         \u001b[1;31m# Katalog docelowy.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0mtrain_dir\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[1;31m# Zmienia rozdzielczość wszystkich obrazów na 150150.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mtarget_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m150\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m150\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_dir' is not defined"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,)\n",
    "\n",
    "# Pamiętaj o tym, że nie powinno się modyfikować danych walidacyjnych!\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        # Katalog docelowy.\n",
    "        train_dir,\n",
    "        # Zmienia rozdzielczość wszystkich obrazów na 150150.\n",
    "        target_size=(150, 150),\n",
    "        batch_size=32,\n",
    "        # Korzystamy z funkcji straty binarnej entropii krzyżowej, a więc potrzebujemy etykiet w formie binarnej.\n",
    "        class_mode='binary')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        validation_dir,\n",
    "        target_size=(150, 150),\n",
    "        batch_size=32,\n",
    "        class_mode='binary')\n",
    "\n",
    "history = model.fit_generator(\n",
    "      train_generator,\n",
    "      steps_per_epoch=100,\n",
    "      epochs=100,\n",
    "      validation_data=validation_generator,\n",
    "      validation_steps=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zapiszmy utworzony model — będziemy z niego korzystać ponownie w podrozdziale 5.4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('cats_and_dogs_small_2.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wygenerujmy ponownie wykresy parametrów modelu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\CSComarch\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\CSComarch\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\CSComarch\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\CSComarch\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:131: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\CSComarch\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\CSComarch\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From C:\\Users\\CSComarch\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\CSComarch\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\CSComarch\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\CSComarch\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\CSComarch\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\CSComarch\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\CSComarch\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\CSComarch\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\CSComarch\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From C:\\Users\\CSComarch\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\CSComarch\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('cats_and_dogs_small_2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-b5418ec427d4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0macc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'acc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'http://localhost:8888/notebooks/Documents/delepy/5.2.ipynb#val_acc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mval_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "acc = history.history['acc']\n",
    "val_acc = history.history['http://localhost:8888/notebooks/Documents/delepy/5.2.ipynb#val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Dokladnosc trenowania')\n",
    "plt.plot(epochs, val_acc, 'b', label='Dokladnosc walidacji')\n",
    "plt.title('Dokladnosc trenowania i walidacji')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Strata trenowania')\n",
    "plt.plot(epochs, val_loss, 'b', label='Strata walidacji')\n",
    "plt.title('Strata trenowania i walidacji')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dzięki zastosowaniu technik augmentacji danych i odrzucania nie mamy już problemu nadmiernego dopasowania. Krzywe trenowania i walidacji mają podobny przebieg. Dokładność osiągnęła poziom 82%, a więc w skali względnej uległa poprawie o 15% w stosunku do początkowej wersji modelu.\n",
    "\n",
    "Poprzez dalsze stosowanie technik regularyzacji i dostrajanie parametrów sieci, takich jak liczba filtrów poszczególnych warstw konwolucji lub liczba warstw sieci, możesz zbliżyć się do dokładności na poziomie 86 – 87%. Uzyskanie wyższej dokładności w wyniku trenowania własnej sieci konwolucyjnej od podstaw byłoby trudne, ponieważ dysponujemy małą ilością danych. W celu dalszego zwiększania dokładności musimy skorzystać z wytrenowanego wcześniej modelu. Technika ta będzie tematem przewodnim dwóch kolejnych podrozdziałów."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
