{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# korekta błędu w Keras po zmianie w bibliotece numpy\n",
    "import numpy as np\n",
    "np_load_old = np.load\n",
    "np.load = lambda *a, **k: np_load_old(*a, allow_pickle=True, **k)\n",
    "\n",
    "# wyłączenie ostrzeżeń\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\"\n",
    "\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.2.4'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Korzystanie z wcześniej wytrenowanej konwolucyjnej sieci neuronowej\n",
    "\n",
    "Korzystanie z uprzednio wytrenowanej sieci jest częstą praktyką uczenia głębokiego, która charakteryzuje się dużą skutecznością w przypadku uczenia się na małych zbiorach obrazów. Wytrenowana uprzednio sieć jest zapisaną siecią, która została wcześniej wytrenowana na dużym zbiorze danych (zwykle trenowanie to miało miejsce podczas wykonywania zadania klasyfikacji dużego zbioru obrazów). Jeżeli oryginalny zbiór danych był na tyle duży i na tyle ogólny, to przestrzenna hierarchia cech wyuczona przez wytrenowany wcześniej moduł może skutecznie odgrywać rolę ogólnego modelu przetwarzania obrazu — cechy tej sieci mogą przydawać się podczas rozwiązywania różnych problemów przetwarzania obrazu pomimo tego, że nowe problemy wymagają rozpoznawania zupełnie innych klas, niż miało to miejsce w przypadku oryginalnego zadania. Sieć może zostać wytrenowana np. na zbiorze ImageNet (klasami są w nim głównie zwierzęta i przedmioty codziennego użytku), a następnie użyta w celu wykonania zadania takiego jak identyfikacja mebli widocznych na zupełnie innych zdjęciach. Możliwość stosowania wytrenowanych sieci do rozwiązywania różnych problemów jest ogromną przewagą uczenia głębokiego nad starszymi algorytmami uczenia płytkiego. Dzięki tej technice uczenie głębokie może byś stosowane skutecznie w pracy z małymi zbiorami danych.\n",
    "\n",
    "Chcemy skorzystać z dużej sieci konwolucyjnej wytrenowanej na zbiorze ImgeNet (1,4 miliona obrazów podzielonych na 1000 klas). Zbiór ImageNet obejmuje wiele klas zwierząt, w tym różne rasy psów i kotów, a więc wytrenowany na nim model prawdopodobnie sprawdzi się dobrze w przypadku problemu klasyfikacji zdjęć na zdjęcia psów i zdjęcia kotów.\n",
    "\n",
    "Skorzystamy z architektury VGG16 opracowanej przez Karen Simonyan i Andrew Zissermana w 2014 r. To prosta i często stosowana architektura sieci uczących się na zbiorze ImageNet . Co prawda jest to dość stary model, który odbiega dość mocno od najnowszych i najbardziej zaawansowanych modeli, ale znamy już jego architekturę i jesteśmy w stanie zrozumieć jego działanie bez wprowadzania żadnych nowych koncepcji. W przyszłości z pewnością będziesz korzystać z modeli o nazwach pokroju VGG, ResNet, Inception, Inception-ResNet, Xception itd. Człony te występują w nazwach wielu architektur stosowanych w przetwarzaniu obrazu.\n",
    "\n",
    "Istnieją dwie techniki stosowania wytrenowanej wcześniej sieci: ekstrakcja cech i dostrajanie. Opiszę oba rozwiązania, ale zacznę od ekstrakcji cech."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ekstrakcja cech\n",
    "\n",
    "Ekstrakcja cech polega na korzystaniu z reprezentacji wyuczonej przez sieć wcześniej w celu dokonania ekstrakcji interesujących nas cech z nowych próbek. Cechy te są następnie przepuszczane przez nowy klasyfikator trenowany od podstaw.\n",
    "\n",
    "Przypominam, że konwolucyjne sieci neuronowe stosowane w problemach klasyfikacji obrazów składają się z dwóch elementów serii warstw łączących (warstw pooling) i warstw konwolucyjnych. Na końcu sieci znajduje się gęsto połączony klasyfikator. Pierwsza część sieci określana jest mianem konwolucyjnej podstawy modelu. W przypadku sieci konwolucyjnej ekstrakcja cech polega na przyjrzeniu się konwolucyjnej podstawie wcześniej wytrenowanej sieci, przepuszczeniu przez nią nowych danych i wytrenowaniu nowego klasyfikatora na bazie wyjścia tej sieci:\n",
    "\n",
    "![swapping FC classifiers](img\\5_3a.png)\n",
    "\n",
    "Dlaczego korzystamy ponownie tylko z konwolucyjnej bazy? Czy moglibyśmy skorzystać znów z gęsto połączonego klasyfikatora? Zwykle należy tego unikać. Wynika to z tego, że reprezentacje wyuczone przez konwolucyjną bazę są zwykle bardziej ogólne, a więc można z nich skorzystać również podczas pracy nad innym problemem: mapy cech konwolucyjnej sieci neuronowej przedstawiają ogólne koncepcje, które nadają się do rozwiązywania różnych problemów związanych z przetwarzaniem obrazu. Reprezentacje wyuczone przez klasyfikator są specyficzne dla danego zestawu klas, na których trenowano model — będą zawierać tylko informacje pozwalające na określenie prawdopodobieństwa tego, czy dana klasa jest widoczna na obrazie. Ponadto reprezentacje gęsto połączonych warstw nie zawierają żadnych informacji o miejscu umieszczenia rozpoznawanych obiektów na obrazie wejściowym — warstwy te nie polegają na przestrzeni, a informacje o położeniu obiektu znajdują się tylko w konwolucyjnej mapie cech. W przypadku problemów, w których położenie obiektu ma znaczenie praktyczne, gęsto połączone cechy są bezużyteczne.\n",
    "\n",
    "Zauważ, że poziom ogólności (możliwości ponownego zastosowania) reprezentacji wyciągniętych przez określone warstwy konwolucyjne zależy od głębokości warstwy modelu. Warstwy występujące w modelu wcześniej dokonują ekstrakcji lokalnych wysoce ogólnych map cech (np. krawędzi, kolorów i tekstur), a warstwy wyższe dokonują ekstrakcji bardziej abstrakcyjnych koncepcji (rozpoznają np. „ucho kota” lub „oko psa”). Jeżeli nowy zbiór danych różni się bardzo od zbioru, na którym model był początkowo trenowany, to prawdopodobnie lepiej będzie, jak skorzystasz z kilku pierwszych warstw tego modelu w celu przeprowadzenia ekstrakcji cech i nie skorzystasz z całej konwolucyjnej bazy.\n",
    "\n",
    "W naszym przypadku w zbiorze ImageNet wyodrębniono wiele klas zdjęć psów i kotów, a prawdopodobnie warto jest skorzystać z informacji umieszczonych w gęsto połączonych warstwach oryginalnego modelu, ale nie zrobimy tego w celu zaimplementowania bardziej ogólnego rozwiązania, w którym zestaw klas nowego problemu nie nakłada się na zestaw klas oryginalnego modelu. Zróbmy to w praktyce, korzystając z konwolucyjnej bazy sieci CGG16 wytrenowanej na zbiorze ImageNet w celu ekstrakcji cech ze zdjęć psów i kotów, a następnie wytrenujmy klasyfikator dzielący obrazy na zdjęcia psów i zdjęcia kotów."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model VGG16 jest jednym z modeli dołączonych do pakietu Keras. Można go zaimportować z modułu keras.applications. Oto lista modeli klasyfikujących obrazy dostępnych w tym module (wszystkie te modele zostały wytrenowane na zbiorze ImageNet):\n",
    "\n",
    "* Xception\n",
    "* InceptionV3\n",
    "* ResNet50\n",
    "* VGG16\n",
    "* VGG19\n",
    "* MobileNet\n",
    "\n",
    "Utwórzmy instancję modelu VGG16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\CSComarch\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\CSComarch\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\CSComarch\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\CSComarch\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\CSComarch\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\CSComarch\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\CSComarch\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\CSComarch\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\CSComarch\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\CSComarch\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.applications import VGG16\n",
    "\n",
    "conv_base = VGG16(weights='imagenet',\n",
    "                  include_top=False,\n",
    "                  input_shape=(150, 150, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do konstruktora modelu przekazaliśmy trzy argumenty:\n",
    "\n",
    "* Argument weights określa punkt kontrolny wag, z którego inicjowany jest model.\n",
    "* Argument include_top określa to, czy do górnej części sieci dołączony zostanie gęsto połączony klasyfikator. Domyślnie klasyfikator ten odpowiada za 1000 klas zbioru ImageNet. Mamy zamiar korzystać z własnego klasyfikatora rozpoznającego dwie klasy (psy i koty), a więc nie chcemy dołączać wytrenowanego wcześniej klasyfikatora.\n",
    "* Argument input_shape określa kształt tensorów obrazów, które będą kierowane do wejścia sieci. Argument ten nie musi być definiowany — jeżeli go nie określisz, to sieć będzie w stanie przetworzyć tensory wejściowe o dowolnym kształcie.\n",
    "\n",
    "Oto szczegółowe informacje na temat architektury bazy konwolucyjnej modelu VGG16. Przypomina ona architekturę przedstawionych wcześniej prostych sieci konwolucyjnych:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 150, 150, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 150, 150, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 150, 150, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 75, 75, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 75, 75, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 75, 75, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 37, 37, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 37, 37, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 37, 37, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 37, 37, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 18, 18, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 18, 18, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 9, 9, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "conv_base.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalna mapa cech ma kształt (4, 4, 512). Z tej mapy cech korzystać będzie klasyfikator.\n",
    "\n",
    "Kolejne czynności możemy wykonywać na dwa sposoby:\n",
    "\n",
    "\n",
    "* Możemy uruchomić konwolucyjną bazę na naszym zbiorze danych, zapisać wygenerowane przez nią wartości na dysku w postaci tablicy Numpy, a następnie użyć tej tablicy w charakterze danych wejściowych samodzielnego, gęsto połączonego klasyfikatora podobnego do tego, który został opisany w 1. części tej książki. Rozwiązanie to jest szybkie i łatwo je uruchomić, ponieważ wymaga tylko jednokrotnego uruchomienia konwolucyjnej bazy dla każdego obrazu wejściowego, a konwolucyjna baza jest najbardziej kosztownym obliczeniowo ogniwem potoku przetwarzania danych. Niestety wadą tego rozwiązania jest bark możliwości skorzystania z techniki augmentacji danych.\n",
    "* Możemy rozszerzyć model, który posiadamy (conv_base), dodając do niego warstwy Dense i używając całej tej konstrukcji w celu przetworzenia danych wejściowych. Rozwiązanie to umożliwia korzystanie z augmentacji danych, ponieważ każdy obraz wejściowy przechodzi przez konwolucyjną bazę za każdym razem, gdy jest analizowany przez model. Niestety sprawia to, że technika ta wymaga przeprowadzania bardziej kosztownych obliczeń.\n",
    "\n",
    "Opiszę oba rozwiązania. Na razie skupmy się na kodzie wymaganym w celu zaimplementowania pierwszego z nich — kodzie zapisującym dane wyjściowe bazy conv_base i pozwalającym na użycie ich w charakterze danych wejściowych nowego modelu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "base_dir = 'Data'\n",
    "\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "validation_dir = os.path.join(base_dir, 'validation')\n",
    "test_dir = os.path.join(base_dir, 'test')\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "batch_size = 20\n",
    "\n",
    "def extract_features(directory, sample_count):\n",
    "    features = np.zeros(shape=(sample_count, 4, 4, 512))\n",
    "    labels = np.zeros(shape=(sample_count))\n",
    "    generator = datagen.flow_from_directory(\n",
    "        directory,\n",
    "        target_size=(150, 150),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary')\n",
    "    i = 0\n",
    "    for inputs_batch, labels_batch in generator:\n",
    "        features_batch = conv_base.predict(inputs_batch)\n",
    "        features[i * batch_size : (i + 1) * batch_size] = features_batch\n",
    "        labels[i * batch_size : (i + 1) * batch_size] = labels_batch\n",
    "        i += 1\n",
    "        if i * batch_size >= sample_count:\n",
    "            # Generator zwraca dane w nieskończoność, a więc pętla musi zostać przerwana,\n",
    "            # gdy każdy z obrazów zostanie przeanalizowany jednokrotnie.\n",
    "            break\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_features, train_labels = extract_features(train_dir, 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "validation_features, validation_labels = extract_features(validation_dir, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_features, test_labels = extract_features(test_dir, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wyciągnięte cechy mają obecnie kształt (próbki, 4, 4, 512). Będziemy kierować je do gęsto połączonego klasyfikatora, a więc cechy musimy spłaszczyć do kształtu (próbki, 8192):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = np.reshape(train_features, (2000, 4 * 4 * 512))\n",
    "validation_features = np.reshape(validation_features, (1000, 4 * 4 * 512))\n",
    "test_features = np.reshape(test_features, (1000, 4 * 4 * 512))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teraz możemy zdefiniować gęsto połączony klasyfikator (zwróć uwagę na zastosowanie techniki regularyzacji — porzucania) i wytrenować go na zapisanych przed chwilą danych i etykietach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/30\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 0.6067 - acc: 0.6670 - val_loss: 0.4398 - val_acc: 0.8340\n",
      "Epoch 2/30\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 0.4235 - acc: 0.8025 - val_loss: 0.3543 - val_acc: 0.8750\n",
      "Epoch 3/30\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 0.3515 - acc: 0.8540 - val_loss: 0.3138 - val_acc: 0.8900\n",
      "Epoch 4/30\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 0.3122 - acc: 0.8615 - val_loss: 0.2916 - val_acc: 0.8950\n",
      "Epoch 5/30\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 0.2828 - acc: 0.8850 - val_loss: 0.2763 - val_acc: 0.8900\n",
      "Epoch 6/30\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 0.2587 - acc: 0.8965 - val_loss: 0.2660 - val_acc: 0.8920\n",
      "Epoch 7/30\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 0.2479 - acc: 0.9030 - val_loss: 0.2579 - val_acc: 0.8910\n",
      "Epoch 8/30\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 0.2180 - acc: 0.9190 - val_loss: 0.2588 - val_acc: 0.8980\n",
      "Epoch 9/30\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 0.2124 - acc: 0.9190 - val_loss: 0.2610 - val_acc: 0.8910\n",
      "Epoch 10/30\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 0.2007 - acc: 0.9235 - val_loss: 0.2440 - val_acc: 0.8930\n",
      "Epoch 11/30\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 0.1909 - acc: 0.9325 - val_loss: 0.2424 - val_acc: 0.8930\n",
      "Epoch 12/30\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 0.1798 - acc: 0.9345 - val_loss: 0.2473 - val_acc: 0.8970\n",
      "Epoch 13/30\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 0.1699 - acc: 0.9350 - val_loss: 0.2360 - val_acc: 0.8960\n",
      "Epoch 14/30\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 0.1730 - acc: 0.9355 - val_loss: 0.2379 - val_acc: 0.8990\n",
      "Epoch 15/30\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 0.1616 - acc: 0.9390 - val_loss: 0.2345 - val_acc: 0.9040\n",
      "Epoch 16/30\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 0.1524 - acc: 0.9455 - val_loss: 0.2344 - val_acc: 0.9020\n",
      "Epoch 17/30\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 0.1374 - acc: 0.9505 - val_loss: 0.2344 - val_acc: 0.9010\n",
      "Epoch 18/30\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 0.1357 - acc: 0.9525 - val_loss: 0.2343 - val_acc: 0.9010\n",
      "Epoch 19/30\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 0.1350 - acc: 0.9565 - val_loss: 0.2418 - val_acc: 0.9010\n",
      "Epoch 20/30\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 0.1279 - acc: 0.9550 - val_loss: 0.2335 - val_acc: 0.9020\n",
      "Epoch 21/30\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 0.1284 - acc: 0.9575 - val_loss: 0.2451 - val_acc: 0.8980\n",
      "Epoch 22/30\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 0.1206 - acc: 0.9605 - val_loss: 0.2309 - val_acc: 0.9000\n",
      "Epoch 23/30\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 0.1119 - acc: 0.9690 - val_loss: 0.2340 - val_acc: 0.9020\n",
      "Epoch 24/30\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 0.1117 - acc: 0.9635 - val_loss: 0.2354 - val_acc: 0.9000\n",
      "Epoch 25/30\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 0.1058 - acc: 0.9705 - val_loss: 0.2340 - val_acc: 0.9010\n",
      "Epoch 26/30\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 0.1017 - acc: 0.9700 - val_loss: 0.2403 - val_acc: 0.8990\n",
      "Epoch 27/30\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 0.0969 - acc: 0.9695 - val_loss: 0.2338 - val_acc: 0.9010\n",
      "Epoch 28/30\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 0.0929 - acc: 0.9710 - val_loss: 0.2491 - val_acc: 0.8940\n",
      "Epoch 29/30\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 0.0856 - acc: 0.9775 - val_loss: 0.2360 - val_acc: 0.9010\n",
      "Epoch 30/30\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 0.0889 - acc: 0.9760 - val_loss: 0.2420 - val_acc: 0.8990\n"
     ]
    }
   ],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(256, activation='relu', input_dim=4 * 4 * 512))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer=optimizers.RMSprop(lr=2e-5),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "history = model.fit(train_features, train_labels,\n",
    "                    epochs=30,\n",
    "                    batch_size=20,\n",
    "                    validation_data=(validation_features, validation_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trenowanie przebiega bardzo szybko, ponieważ mamy tylko dwie warstwy Dense — przetworzenie epoki algorytmu zajmuje mniej niż sekundę nawet w przypadku korzystania z CPU.\n",
    "\n",
    "Przeanalizujmy wykresy straty i dokładności trenowania:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Dokladnosc trenowania')\n",
    "plt.plot(epochs, val_acc, 'b', label='Dokladnosc walidacji')\n",
    "plt.title('Dokladnosc trenowania i walidacji')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Strata trenowania')\n",
    "plt.plot(epochs, val_loss, 'b', label='Strata walidacji')\n",
    "plt.title('Strata trenowania i walidacji')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Uzyskaliśmy dokładność na poziomie zbliżonym do 90% — to wynik o wiele lepszy od tego, który uzyskaliśmy w poprzednim podrozdziale, gdy trenowaliśmy model od podstaw. Z wykresów wynika jednak również to, że model niemalże od początku ulega nadmiernemu dopasowaniu pomimo przyjęcia dość dużej wartości współczynnika odrzutu. Wynika to z tego, że nie korzystamy z techniki augmentacji danych, która jest niezbędna, by zapobiegnąć przeuczeniu modelu podczas przetwarzania małych zbiorów obrazów.\n",
    "\n",
    "Teraz przyjrzyjmy się drugiej technice ekstrakcji cech, która jest o wiele wolniejsza i wymaga wykonywania o wiele bardziej skomplikowanych obliczeń, ale umożliwia korzystanie z augmentacji danych podczas trenowania — dokonamy rozbudowy modelu conv_base i przetworzymy dane od początku do końca przy użyciu tego właśnie modelu.\n",
    "\n",
    "Modele mają właściwości warstw, a więc model conv_base może zostać dodany do modelu sekwencyjnego (Sequential), tak jakbyśmy łączyli ze sobą kolejne warstwy sieci."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(conv_base)\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(256, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teraz nasz model charakteryzuje się następującą budową:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 256)               2097408   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 2,097,665\n",
      "Trainable params: 2,097,665\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Konwolucyjna baza sieci VGG16 ma 14 714 688 parametrów (jest to bardzo duża liczba). Klasyfikator dodawany do tej bazy ma 2 miliony parametrów.\n",
    "\n",
    "Zanim skompilujemy i wytrenujemy model, musimy zamrozić bazę konwolucyjną. Zamrażanie warstwy lub zestawu warstw polega na zapobieganiu aktualizacji ich wag w procesie trenowania. Jeżeli tego nie zrobimy, to reprezentacje wyuczone wcześniej przez bazę konwolucyjną zostaną zmodyfikowane podczas trenowania. Warstwy Dense znajdujące się u góry są inicjowane w sposób losowy, co sprawia, że dochodziłoby do dużych zmian wszystkich parametrów sieci, co skutecznie zniszczyłoby wyuczone wcześniej reprezentacje.\n",
    "\n",
    "W pakiecie Keras sieć zamraża się, przypisując wartość False atrybutowi trainable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liczba wag poddawanych trenowaniu przed zamrożeniem bazy: 30\n"
     ]
    }
   ],
   "source": [
    "print('Liczba wag poddawanych trenowaniu '\n",
    "      'przed zamrożeniem bazy:', len(model.trainable_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_base.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liczba wag poddawanych trenowaniu po zamrożeniu bazy: 4\n"
     ]
    }
   ],
   "source": [
    "print('Liczba wag poddawanych trenowaniu '\n",
    "      'po zamrożeniu bazy:', len(model.trainable_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przy takiej konfiguracji trenowane będą tylko wagi z dwóch warstw Dense. Będą to w sumie cztery tensory wag: po dwa tensory na warstwę (główna macierz wag i wektor wartości progowych). Pamiętaj o tym, że w celu wprowadzenia zmian należy najpierw skompilować model. Jeżeli zmienisz możliwość trenowania wag po skompilowaniu modelu, musisz go skompilować jeszcze raz, w przeciwnym razie wprowadzone zmiany zostaną zignorowane.\n",
    "\n",
    "Teraz możemy rozpocząć trenowanie modelu przy takiej samej konfiguracji techniki augmentacji danych, z jakiej korzystaliśmy w poprzednim przykładzie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n",
      "Found 1000 images belonging to 2 classes.\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-6feecac5b29b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     40\u001b[0m       \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_generator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m       \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m       verbose=2)\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[0;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1418\u001b[1;33m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1419\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1420\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[0;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 217\u001b[1;33m                                             class_weight=class_weight)\n\u001b[0m\u001b[0;32m    218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1217\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1218\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1219\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1472\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1473\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "      rescale=1./255,\n",
    "      rotation_range=40,\n",
    "      width_shift_range=0.2,\n",
    "      height_shift_range=0.2,\n",
    "      shear_range=0.2,\n",
    "      zoom_range=0.2,\n",
    "      horizontal_flip=True,\n",
    "      fill_mode='nearest')\n",
    "\n",
    "# Pamiętaj o tym, że dane walidacyjne nie mogą być modyfikowane!\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        # Katalog docelowy.\n",
    "        train_dir,\n",
    "        # Rozdzielczość wszystkich obrazów zostanie zmieniona na 150x150.\n",
    "        target_size=(150, 150),\n",
    "        batch_size=20,\n",
    "        # Korzystamy z funkcji straty w postaci binarnej entropii krzyżowej, a więc potrzebujemy etykiet w formie binarnej.\n",
    "        class_mode='binary')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        validation_dir,\n",
    "        target_size=(150, 150),\n",
    "        batch_size=20,\n",
    "        class_mode='binary')\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=2e-5),\n",
    "              metrics=['acc'])\n",
    "\n",
    "history = model.fit_generator(\n",
    "      train_generator,\n",
    "      steps_per_epoch=100,\n",
    "      epochs=30,\n",
    "      validation_data=validation_generator,\n",
    "      validation_steps=50,\n",
    "      verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('cats_and_dogs_small_3.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ponownie przedstawmy wyniki pracy algorytmu na wykresach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Dokladnosc trenowania')\n",
    "plt.plot(epochs, val_acc, 'b', label='Dokladnosc walidacji')\n",
    "plt.title('Dokladnosc trenowania i walidacji')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Strata trenowania')\n",
    "plt.plot(epochs, val_loss, 'b', label='Strata walidacji')\n",
    "plt.title('Strata trenowania i walidacji')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jak widać, uzyskaliśmy dokładność walidacji sięgającą około 96%. To o wiele lepszy wynik od tego, który uzyskaliśmy przy użyciu małej sieci konwolucyjnej trenowanej od podstaw."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dostrajanie\n",
    "\n",
    "Dostrajanie (patrz rysunek 5.19) jest techniką ponownego stosowania modeli uzupełniającą ekstrakcję cech. Polega ona na odmrażaniu kilku górnych warstw zamrożonej bazy modelu używanej do ekstrakcji cech i trenowanie jej łącznie z nową częścią modelu (w naszym przypadku tą częścią modelu jest w pełni połączony klasyfikator). Proces ten określamy mianem dostrajania, ponieważ modyfikuje on częściowo wytrenowane wcześniej bardziej abstrakcyjne reprezentacje modelu w celu dostosowania ich do bieżącego problemu.\n",
    "\n",
    "![fine-tuning VGG16](img\\5_3b.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stwierdziłem, że musimy zamrozić bazę konwolucji modelu VGG16 aby wytrenować losowo zainicjowany klasyfikator kończący sieć. Z tego samego powodu możliwe jest tylko dostrajanie górnych warstw konwolucyjnej bazy po wytrenowaniu klasyfikatora. Jeżeli klasyfikator nie byłby jeszcze wytrenowany, to sygnał błędu przepływający przez sieć w trakcie trenowania byłby zbyt duży i doprowadziłby do zniszczenia reprezentacji wyuczonych wcześniej przez dostrajane warstwy. W związku z tym dostrajanie sieci należy przeprowadzać w następujący sposób:\n",
    "\n",
    "* 1) Dodaj samodzielnie zaprojektowaną sieć do końca bazy wytrenowanego już modelu.\n",
    "* 2) Zamróź bazę sieci.\n",
    "* 3) Wytrenuj nową część sieci.\n",
    "* 4) Odmroź niektóre warstwy bazy sieci.\n",
    "* 5) Wytrenuj razem te warstwy oraz nową część sieci.\n",
    "\n",
    "Pierwsze trzy kroki wykonaliśmy podczas ekstrakcji cech. Zajmijmy się krokiem 4. Odmrozimy bazę conv_base, a następnie zamrozimy jej wybrane warstwy.\n",
    "\n",
    "Przypominam, że nasza sieć charakteryzuje się następującą architekturą:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 150, 150, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 150, 150, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 150, 150, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 75, 75, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 75, 75, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 75, 75, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 37, 37, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 37, 37, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 37, 37, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 37, 37, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 18, 18, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 18, 18, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 9, 9, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 0\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "conv_base.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Dostroimy ostatnie trzy warstwy konwolucyjne, a więc wszystkie warstwy aż do warstwy block4_pool powinny zostać zamrożone, a warstwy block5_conv1, block5_conv2 i block5_conv3 powinny dawać się trenować.\n",
    "\n",
    "Dlaczego nie będziemy dostrajać większej liczby warstw? Dlaczego nie dostroić całej bazy konwolucyjnej? Moglibyśmy to zrobić, ale musimy pamiętać o następujących rzeczach:\n",
    "\n",
    "\n",
    "* Wcześniejsze warstwy bazy konwolucyjnej kodują bardziej ogólne, uniwersalne cechy, a wyższe warstwy kodują bardziej wyspecjalizowane cechy. Lepiej jest dostrajać bardziej wyspecjalizowane cechy, ponieważ to od nich zależy przydatność modelu w nowym zastosowaniu. Podczas dostrajania niższych warstw dochodziłoby do szybkiego spadku zwracanych wartości.\n",
    "* Im więcej parametrów jest trenowanych, tym większe jest ryzyko nadmiernego dopasowania. Baza konwolucyjna ma 15 milionów parametrów, a trenowanie tak dużej liczby parametrów na niewielkim zbiorze danych, którym dysponujemy, byłoby ryzykowne.\n",
    "\n",
    "W związku z tym w naszym przypadku dobrą strategią jest dostrajanie tylko dwóch lub trzech górnych warstw bazy konwolucyjnej. Zróbmy to, zaczynając od miejsca, w którym skończyliśmy w poprzednim przykładzie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_base.trainable = True\n",
    "\n",
    "set_trainable = False\n",
    "for layer in conv_base.layers:\n",
    "    if layer.name == 'block5_conv1':\n",
    "        set_trainable = True\n",
    "    if set_trainable:\n",
    "        layer.trainable = True\n",
    "    else:\n",
    "        layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teraz możemy rozpocząć dostrajanie sieci. Zrobimy to za pomocą algorytmu optymalizującego RMSProp przy bardzo niskiej wartości parametru uczenia. Wybranie niskiej wartości wynika z tego, że chcemy minimalizować modyfikacje reprezentacji trzech dostrajanych warstw. Zbyt duże zmiany tych wartości mogłyby zaszkodzić reprezentacjom danych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=1e-5),\n",
    "              metrics=['acc'])\n",
    "\n",
    "history = model.fit_generator(\n",
    "      train_generator,\n",
    "      steps_per_epoch=100,\n",
    "      epochs=100,\n",
    "      validation_data=validation_generator,\n",
    "      validation_steps=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('cats_and_dogs_small_4.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wygenerujmy wykresy, korzystając z napisanego wcześniej kodu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Dokladnosc trenowania')\n",
    "plt.plot(epochs, val_acc, 'b', label='Dokladnosc walidacji')\n",
    "plt.title('Dokladnosc trenowania i walidacji')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Strata trenowania')\n",
    "plt.plot(epochs, val_loss, 'b', label='Strata walidacji')\n",
    "plt.title('Strata trenowania i walidacji')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Krzywe te wyglądają dość chaotycznie. W celu zwiększenia ich czytelności możemy je wygładzić zastępując poszczególne wartości straty i dokładności wykładniczą średnią ruchomą tych parametrów. Oto prosta funkcja pomocnicza, która pozwoli na wygenerowanie bardziej czytelnych wykresów:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_curve(points, factor=0.8):\n",
    "  smoothed_points = []\n",
    "  for point in points:\n",
    "    if smoothed_points:\n",
    "      previous = smoothed_points[-1]\n",
    "      smoothed_points.append(previous * factor + point * (1 - factor))\n",
    "    else:\n",
    "      smoothed_points.append(point)\n",
    "  return smoothed_points\n",
    "\n",
    "plt.plot(epochs,\n",
    "         smooth_curve(acc), 'bo', label='Wygladzona dokladnosc trenowania')\n",
    "plt.plot(epochs,\n",
    "         smooth_curve(val_acc), 'b', label='Wygladzona dokladnosc walidacji')\n",
    "plt.title('Dokladnosc trenowania i walidacji')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs,\n",
    "         smooth_curve(loss), 'bo', label='Wygladzona strata trenowania')\n",
    "plt.plot(epochs,\n",
    "         smooth_curve(val_loss), 'b', label='Wygladzona strata walidacji')\n",
    "plt.title('Strata trenowania i walidacji')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Krzywa dokładności walidacji wygląda wyraźniej. Widać ładny 1-procentowy bezwzględny wzrost dokładności (z 96% na ponad 97%).\n",
    "\n",
    "Zauważ, że krzywa straty nie wykazuje żadnej realnej poprawy (tak naprawdę wynika z niej pogorszenie parametru pracy modelu). Dlaczego dokładność jest stabilna, a nawet poprawiła się, jeżeli wartości straty nie spadają? Odpowiedź jest prosta: widzimy średnią punktowych wartości straty, a dokładność zależy od rozkładu wartości straty (nie ich średniej). Dzieje się tak, ponieważ dokładność jest binarną wartością progową prawdopodobieństwa klas przewidywanych przez model. Model może działać lepiej pomimo tego, że nie odzwierciedlają tego średnie wartości straty.\n",
    "\n",
    "\n",
    "Na koniec możemy sprawdzić działanie modelu na danych testowych:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_generator = test_datagen.flow_from_directory(\n",
    "        test_dir,\n",
    "        target_size=(150, 150),\n",
    "        batch_size=20,\n",
    "        class_mode='binary')\n",
    "\n",
    "test_loss, test_acc = model.evaluate_generator(test_generator, steps=50)\n",
    "print('dokładnosc podczas testowania:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Uzyskamy testową dokładność na poziomie 97%. Gdybyśmy wzięli udział w konkursie Kaggle dotyczącym tego zbioru danych, to uzyskalibyśmy jeden z najwyższych wyników. Dzięki nowoczesnym technikom uczenia głębokiego udało nam się uzyskać taki wynik, dysponując tylko niewielką częścią treningowego zbioru danych (około 10% danych). Możliwość trenowania modelu na zbiorze 20 000 próbek to coś zupełnie innego niż trenowanie modelu na 2000 próbek"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wnioski\n",
    "\n",
    "Oto wnioski, które należy wynieść z dwóch ostatnich podrozdziałów:\n",
    "\n",
    "* Konwolucyjne sieci neuronowe są najlepszymi modelami uczenia maszynowego do zadań związanych z przetwarzaniem obrazu. Można je trenować od podstaw nawet na małych zbiorach danych i uzyskiwać przy tym sensowne wyniki.\n",
    "* Głównym problemem podczas pracy z małymi zbiorami danych jest nadmierne dopasowanie modelu. Problem ten podczas pracy z danymi będącymi obrazami można rozwiązać za pomocą techniki augmentacji danych.\n",
    "* Ekstrakcja cech umożliwia łatwe użycie utworzonej wcześniej konwolucyjnej sieci neuronowej w celu rozwiązania nowego problemu. Technika ta jest szczególnie przydatna podczas pracy z małymi zbiorami obrazów.\n",
    "* Efekty ekstrakcji cech można wzmocnić techniką dostrajania, która polega na przystosowywaniu wyuczonej wcześniej reprezentacji danych do nowego problemu. Zabieg ten zwiększa nieco skuteczność pracy modelu.\n",
    "\n",
    "Teraz dysponujesz już solidnym zestawem narzędzi przeznaczonych do rozwiązywania problemów klasyfikacji obrazów, które to narzędzia sprawdzają się szczególnie dobrze podczas pracy z małymi zbiorami danych."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
