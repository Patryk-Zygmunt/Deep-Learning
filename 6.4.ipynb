{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# korekta błędu w Keras po zmianie w bibliotece numpy\n",
    "import numpy as np\n",
    "np_load_old = np.load\n",
    "np.load = lambda *a, **k: np_load_old(*a, allow_pickle=True, **k)\n",
    "\n",
    "# wyłączenie ostrzeżeń\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Konwolucyjne sieci neuronowe i przetwarzanie sekwencji\n",
    "\n",
    "\n",
    "\n",
    "## Implementacja jednowymiarowej sieci konwolucyjnej\n",
    "\n",
    "W pakiecie Keras jednowymiarowe sieci konwolucyjne zostały zaimplementowane w postaci warstwy Conv1D. Na wejściu warstwa ta przyjmuje trójwymiarowe tensory o kształcie (próbki, czas, cechy) i zwraca trójwymiarowe tensory o takim samym kształcie. Okno konwolucyjne jest jednowymiarowym oknem poruszającym się wzdłuż osi czasu (osi tensora wejściowego oznaczonej numerem 1).\n",
    "\n",
    "Zbudujmy prostą jednowymiarową sieć konwolucyjną i spróbujmy użyć jej do analizy sentymentu recenzji wchodzących w skład zbioru IMDB. Przypominam kod importujący i przetwarzający wstępnie dane tego zbioru:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "max_features = 10000  # Liczba słów traktowanych jako cechy.\n",
    "max_len = 500  # Ucina recenzje po tej liczbie słów należących do zbioru max_features słów najczęściej występujących w zbiorze.\n",
    "\n",
    "print('Ładowanie danych...')\n",
    "(input_train, y_train), (input_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(input_train), 'sekwencje treningowe')\n",
    "print(len(input_test), 'sekwencje testowe')\n",
    "\n",
    "print('Sekwencje (próbki x czas)')\n",
    "input_train = sequence.pad_sequences(input_train, maxlen=max_len)\n",
    "input_test = sequence.pad_sequences(input_test, maxlen=max_len)\n",
    "print('Kształt obiektu input_train:', input_train.shape)\n",
    "print('Kształt obiektu input_test:', input_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Jednowymiarowe sieci konwolucyjne mają taką samą strukturę jak ich dwuwymiarowe odpowiedniki, z których korzystaliśmy w rozdziale 5: składają się ze stosu warstw Conv1D i MaxPooling1D, a na ich końcu znajduje się globalna warstwa łącząca lub warstwa spłaszczająca (Flatten) zamieniająca trójwymiarowe obiekty wyjściowe w obiekty dwuwymiarowe, co pozwala na dodanie do modelu klasyfikacji lub regresji przynajmniej jednej warstwy Dense.\n",
    "Jedną z różnic między tymi typami sieci jest to, że w przypadku jednowymiarowych sieci konwolucyjnych możemy pozwolić sobie na użycie większych okien konwolucji. W przypadku dwuwymiarowej warstwy konwolucji okno konwolucji o wymiarach 3x3 zawiera 3x3 = 9 wektorów cech, a w przypadku warstwy jednowymiarowej konwolucji okno konwolucji o rozmiarze równym 3 składa się tylko z 3 wektorów cech. W związku z tym możemy swobodnie pozwolić sobie na korzystanie z jednowymiarowych okien konwolucji o rozmiarze równym 7 lub 9.\n",
    "\n",
    "Oto przykładowa jednowymiarowa sieć konwolucyjna przetwarzająca zbiór danych IMDB:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(max_features, 128, input_length=max_len))\n",
    "model.add(layers.Conv1D(32, 7, activation='relu'))\n",
    "model.add(layers.MaxPooling1D(5))\n",
    "model.add(layers.Conv1D(32, 7, activation='relu'))\n",
    "model.add(layers.GlobalMaxPooling1D())\n",
    "model.add(layers.Dense(1))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer=RMSprop(lr=1e-4),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "history = model.fit(input_train, y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=128,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dokładność walidacji jest nieco mniejsza od tej, którą uzyskał model sieci LSTM, ale trenowanie bieżącego modelu trwało znacznie szybciej (dotyczy to pracy na układzie CPU, a także układzie GPU). Oczywiście dokładny przyrost prędkości wykonywania obliczeń zależy od konfiguracji sprzętowej. Możemy zapisać ten model, ograniczając czas jego wykonywania do ośmiu epok, a następnie sprawdzić jego działanie na zbiorze testowym. To z pewnością przekonujący dowód na to, że jednowymiarowa sieć konwolucyjna jest szybszą i tańszą alternatywą stosowania sieci rekurencyjnej w przypadku problemu analizy sentymentu na poziomie słów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Dokladnosc trenowania')\n",
    "plt.plot(epochs, val_acc, 'b', label='Dokladnosc walidacji')\n",
    "plt.title('Dokladnosc trenowania i walidacji')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Strata trenowania')\n",
    "plt.plot(epochs, val_loss, 'b', label='Strata walidacji')\n",
    "plt.title('Strata trenowania i walidacji')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Łączenie sieci konwolucyjnych i rekurencyjnych w celu przetworzenia długich sekwencji\n",
    "\n",
    "\n",
    "Jednowymiarowe sieci konwolucyjne przetwarzają sekwencje wejściowe w sposób niezależny. W przeciwieństwie do siei rekurencyjnych nie są wrażliwe na kolejność obserwacji (poza skalą lokalną określaną przez rozmiar okna konwolucji). Oczywiście w celu rozpoznana bardziej rozciągniętych prawidłowości możemy utworzyć stos wielu warstw konwolucyjnych i warstw łączących. W ten sposób górne warstwy będą analizować dłuższe fragmenty danych wejściowych, ale jest to słaby sposób na wprowadzenie wrażliwości modelu na kolejność obserwacji. Sprawdźmy w praktyce tę słabość modelu podczas prognozowania temperatury (w tym problemie kolejność jest bardzo ważna dla otrzymania prawidłowych prognoz). W poniższym przykładzie korzystamy ponownie ze zdefiniowanych wcześniej zmiennych loat_data, train_gen, val_gen i val_steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# W poniższym przykładzie korzystamy ponownie ze zdefiniowanych\n",
    "# wcześniej zmiennych loat_data, train_gen, val_gen i val_steps.\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "data_dir = ''\n",
    "fname = os.path.join(data_dir, 'jena_climate_2009_2016.csv')\n",
    "\n",
    "f = open(fname)\n",
    "data = f.read()\n",
    "f.close()\n",
    "\n",
    "lines = data.split('\\n')\n",
    "header = lines[0].split(',')\n",
    "lines = lines[1:]\n",
    "\n",
    "float_data = np.zeros((len(lines), len(header) - 1))\n",
    "for i, line in enumerate(lines):\n",
    "    values = [float(x) for x in line.split(',')[1:]]\n",
    "    float_data[i, :] = values\n",
    "    \n",
    "mean = float_data[:200000].mean(axis=0)\n",
    "float_data -= mean\n",
    "std = float_data[:200000].std(axis=0)\n",
    "float_data /= std\n",
    "\n",
    "def generator(data, lookback, delay, min_index, max_index,\n",
    "              shuffle=False, batch_size=128, step=6):\n",
    "    if max_index is None:\n",
    "        max_index = len(data) - delay - 1\n",
    "    i = min_index + lookback\n",
    "    while 1:\n",
    "        if shuffle:\n",
    "            rows = np.random.randint(\n",
    "                min_index + lookback, max_index, size=batch_size)\n",
    "        else:\n",
    "            if i + batch_size >= max_index:\n",
    "                i = min_index + lookback\n",
    "            rows = np.arange(i, min(i + batch_size, max_index))\n",
    "            i += len(rows)\n",
    "\n",
    "        samples = np.zeros((len(rows),\n",
    "                           lookback // step,\n",
    "                           data.shape[-1]))\n",
    "        targets = np.zeros((len(rows),))\n",
    "        for j, row in enumerate(rows):\n",
    "            indices = range(rows[j] - lookback, rows[j], step)\n",
    "            samples[j] = data[indices]\n",
    "            targets[j] = data[rows[j] + delay][1]\n",
    "        yield samples, targets\n",
    "        \n",
    "lookback = 1440\n",
    "step = 6\n",
    "delay = 144\n",
    "batch_size = 128\n",
    "\n",
    "train_gen = generator(float_data,\n",
    "                      lookback=lookback,\n",
    "                      delay=delay,\n",
    "                      min_index=0,\n",
    "                      max_index=200000,\n",
    "                      shuffle=True,\n",
    "                      step=step, \n",
    "                      batch_size=batch_size)\n",
    "val_gen = generator(float_data,\n",
    "                    lookback=lookback,\n",
    "                    delay=delay,\n",
    "                    min_index=200001,\n",
    "                    max_index=300000,\n",
    "                    step=step,\n",
    "                    batch_size=batch_size)\n",
    "test_gen = generator(float_data,\n",
    "                     lookback=lookback,\n",
    "                     delay=delay,\n",
    "                     min_index=300001,\n",
    "                     max_index=None,\n",
    "                     step=step,\n",
    "                     batch_size=batch_size)\n",
    "\n",
    "# Liczba kroków pobierania danych z obiektu val_gen, \n",
    "# przy której przetworzony zostanie cały walidacyjny zbiór danych.\n",
    "val_steps = (300000 - 200001 - lookback) // batch_size\n",
    "\n",
    "# Liczba kroków pobierania danych z obiektu test_gen, \n",
    "# przy której przetworzony zostanie cały testowy zbiór danych.\n",
    "test_steps = (len(float_data) - 300001 - lookback) // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Conv1D(32, 5, activation='relu',\n",
    "                        input_shape=(None, float_data.shape[-1])))\n",
    "model.add(layers.MaxPooling1D(3))\n",
    "model.add(layers.Conv1D(32, 5, activation='relu'))\n",
    "model.add(layers.MaxPooling1D(3))\n",
    "model.add(layers.Conv1D(32, 5, activation='relu'))\n",
    "model.add(layers.GlobalMaxPooling1D())\n",
    "model.add(layers.Dense(1))\n",
    "\n",
    "model.compile(optimizer=RMSprop(), loss='mae')\n",
    "history = model.fit_generator(train_gen,\n",
    "                              steps_per_epoch=500,\n",
    "                              epochs=20,\n",
    "                              validation_data=val_gen,\n",
    "                              validation_steps=val_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oto wykres średniego bezwzględnego błędu trenowania i walidacji:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(loss))\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Strata trenowania')\n",
    "plt.plot(epochs, val_loss, 'b', label='Strata walidacji')\n",
    "plt.title('Strata trenowania i walidacji')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Średni bezwzględny błąd walidacji utrzymuje się na poziomie około 40%, a więc model ten nie jest w stanie pokonać nawet zdefiniowanego wcześniej punktu odniesienia. Dzieje się tak, ponieważ sieć konwolucyjna poszukuje zależności w całych danych szeregu czasowego bez wiedzy o tym, w jakim czasie zaobserwowano przetwarzane wartości (nie zwraca uwagi na to, czy są one zbliżone bardziej do początku, czy do końca analizowanego zbioru). W tym problemie nowsze dane powinny być interpretowane w inny sposób od danych z dalszej przeszłości, a więc sieć konwolucyjna nie jest w stanie wygenerować sensownych wyników. To ograniczenie sieci konwolucyjnych nie jest problematyczne w przypadku zbioru IMDB, ponieważ słowa świadczące o tym, że dana recenzja jest pochlebna lub negatywna, mogą znajdować się w dowolnym miejscu analizowanych zdań.\n",
    "\n",
    "Sposobem na połączenie szybkości i lekkości sieci konwolucyjnych z braniem pod uwagę kolejności przez sieci rekurencyjne jest zastosowanie jednowymiarowej sieci konwolucyjnej w roli mechanizmu przetwarzającego wstępnie dane kierowane do sieci rekurencyjnej. Rozwiązanie takie sprawdza się szczególnie dobrze podczas pracy z długimi sekwencjami, których długość uniemożliwia ich sensowne przetworzenie za pomocą samych sieci rekurencyjnych (piszę tu o sekwencjach składających się nawet z tysięcy kroków). Sieć konwolucyjna zamieni długą sekwencję wejściową w krótsze sekwencje cech wyższego poziomu, które można następnie skierować do warstwy rekurencyjnej."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Technika ta jest na tyle mało znana, że rzadko spotyka się ją w artykułach naukowych i praktycznych zastosowaniach. Jest jednak na tyle skuteczna, że powinna być bardziej popularna. Spróbujmy użyć jej w celu rozwiązania problemu prognozowania temperatury. Rozwiązanie to pozwala na przetwarzanie o wiele dłuższych sekwencji, a więc możemy przyglądać się danym z dalszej przeszłości poprzez zwiększenie wartości parametru lookback generatora danych lub przyglądać się szeregom czasowym o wyższej rozdzielczości poprzez zmniejszenie wartości parametru step generatora danych. W zaprezentowanym przykładzie zdecydowałem się na zmniejszenie o połowę parametru step, co wydłużyło dwukrotnie analizowane szeregi czasowe (pracujemy z próbkami temperatury odczytywanej w odstępie 30 minut). Ponownie korzystam ze zdefiniowanej wcześniej funkcji generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wcześniej parametr ten przyjmował wartość równą 6 (1 obserwacja na godzinę). \n",
    "# Teraz przyjmuje wartość 3 (1 obserwacja na 30 minut).\n",
    "step = 3\n",
    "lookback = 720  # Bez zmian.\n",
    "delay = 144 # Bez zmian.\n",
    "\n",
    "train_gen = generator(float_data,\n",
    "                      lookback=lookback,\n",
    "                      delay=delay,\n",
    "                      min_index=0,\n",
    "                      max_index=200000,\n",
    "                      shuffle=True,\n",
    "                      step=step)\n",
    "val_gen = generator(float_data,\n",
    "                    lookback=lookback,\n",
    "                    delay=delay,\n",
    "                    min_index=200001,\n",
    "                    max_index=300000,\n",
    "                    step=step)\n",
    "test_gen = generator(float_data,\n",
    "                     lookback=lookback,\n",
    "                     delay=delay,\n",
    "                     min_index=300001,\n",
    "                     max_index=None,\n",
    "                     step=step)\n",
    "val_steps = (300000 - 200001 - lookback) // 128\n",
    "test_steps = (len(float_data) - 300001 - lookback) // 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teraz możemy stworzyć model. Połączymy dwie warstwy Conv1D z warstwą GRU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(layers.Conv1D(32, 5, activation='relu',\n",
    "                        input_shape=(None, float_data.shape[-1])))\n",
    "model.add(layers.MaxPooling1D(3))\n",
    "model.add(layers.Conv1D(32, 5, activation='relu'))\n",
    "model.add(layers.GRU(32, dropout=0.1, recurrent_dropout=0.5))\n",
    "model.add(layers.Dense(1))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer=RMSprop(), loss='mae')\n",
    "history = model.fit_generator(train_gen,\n",
    "                              steps_per_epoch=500,\n",
    "                              epochs=20,\n",
    "                              validation_data=val_gen,\n",
    "                              validation_steps=val_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(loss))\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Strata trenowania')\n",
    "plt.plot(epochs, val_loss, 'b', label='Strata walidacji')\n",
    "plt.title('Strata trenowania i walidacji')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przyglądając się wykresowi straty walidacyjnej, możemy stwierdzić, że taka konfiguracja modelu jest słabsza od samej warstwy GRU poddanej regularyzacji, ale działa znacznie szybciej. Analizuje dwa razy więcej danych, co w tym przypadku nie jest zabiegiem szczególnie przydatnym, ale w innych zbiorach danych może przynieść znaczną poprawę pracy algorytmu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wnioski\n",
    "\n",
    "Oto wnioski, które należy wynieść z tego podrozdziału:\n",
    "\n",
    "* Jednowymiarowe konwolucyjne sieci neuronowe doskonale nadają się do przetwarzania danych szeregów czasowych, tak jak dwuwymiarowe sieci konwolucyjne doskonale nadają się do przetwarzania wzorców zakodowanych w płaszczyznach dwuwymiarowych obrazów. W przypadku niektórych problemów sieci te stanowią szybszą alternatywę sieci rekurencyjnych. Dotyczy to szczególnie zadań związanych z przetwarzaniem języka naturalnego.\n",
    "* Zwykle jednowymiarowe sieci konwolucyjne mają strukturę przypominającą strukturę ich dwuwymiarowych odpowiedników stosowanych do przetwarzania obrazu: składają się ze stosu warstw Conv1D i MaxPooling1D (na ich końcu stosowane są warstwy wykonujące operacje globalnego sumowania lub spłaszczania).\n",
    "* •\tUżywanie rekurencyjnych sieci neuronowych do przetwarzania długich sekwencji wiąże się z dużym wydatkiem obliczeniowym, a jednowymiarowe sieci konwolucyjne wymagają o wiele prostszych obliczeń. W związku z tym warto używać jednowymiarowych sieci konwolucyjnych w roli mechanizmów wstępnie przetwarzających dane przed skierowaniem ich do warstw sieci rekurencyjnej. Takie rozwiązanie skraca analizowaną sekwencję, a warstwy rekurencyjne zajmują się tylko przydatnymi reprezentacjami danych."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
