{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# korekta błędu w Keras po zmianie w bibliotece numpy\n",
    "import numpy as np\n",
    "np_load_old = np.load\n",
    "np.load = lambda *a, **k: np_load_old(*a, allow_pickle=True, **k)\n",
    "\n",
    "# wyłączenie ostrzeżeń\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.2.4'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generowanie obrazów przy użyciu wariacyjnych autoenkoderów\n",
    "\n",
    "(...)\n",
    "\n",
    "\n",
    "## Wariacyjne autoenkodery\n",
    "\n",
    "\n",
    "Wariacyjne autoenkodery zostały opracowane jednocześnie przez Kingmę i Wellinga w grudniu 2013 r.  i Rezende, Mohameda i Wierstrę w styczniu 2014 r. Są to generatywne modele sprawdzające się szczególnie w zadaniach edycji obrazu przy użyciu wektorów koncepcyjnych. Wariacyjne autoenkodery to nowoczesne podejście do autoenkoderów — sieci, która ma na celu zakodowanie danych wejściowych w postaci niskopoziomowej niejawnej przestrzeni, a następnie ich dekodowanie — jest to połączenie uczenia głębokiego z wnioskowaniem bayesowskim.\n",
    "\n",
    "Klasyczny autoenkoder obrazu przetwarza obraz wejściowy, mapuje go na niejawną przestrzeń wektorową z zastosowaniem modułu kodera, a następnie dekoduje go do postaci wejściowej o takich samych wymiarach jak obraz oryginalny przy użyciu modułu dekodera. Autoenkoder jest trenowany tak, aby mógł dokonać rekonstrukcji obrazów wejściowych. Nakładając różne ograniczenia na wyjście kodera, można zmusić autoenkoder do uczenia się interesujących nas niejawnych reprezentacji danych. Zwykle kod ma być niskopoziomowy i rzadki (powinien zawierać głównie zera). W takim przypadki koder działa tak, jakby kompresował dane wejściowe do postaci formy składającej się z kilku bitów informacji."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Autoencoder](img\\8_4a.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "W praktyce takie klasyczne autoenkodery nie pozwalają na uzyskanie szczególnie przydatnej przestrzeni o ładnej strukturze. Nie są one również zbyt wydajne w roli mechanizmów kompresji danych. W związku z tym wyszły one z użycia. Technika kodowania VAE usprawniła działanie autoenkoderów dodając do nich nieco magii statystyki, dzięki której są one w stanie uczyć się ciągłych przestrzeni charakteryzujących się określoną strukturą. W ten sposób powstało solidne narzędzie służące do generowania obrazów.\n",
    "\n",
    "Koder VAE zamiast kompresować obraz wejściowy do określonej formy kodu niejawnej przestrzeni, zamienia obraz w parametry rozkładu statystycznego: średnią i wariancję. Oznacza to założenie, że obraz wejściowy został wygenerowany przez proces statystyczny i podczas kodowania i dekodowania należy wziąć pod uwagę losowość tego procesu. Koder VAE korzysta następnie z parametrów średniej i wariancji w celu losowego próbkowania jednego elementu rozkładu i zdekodowania go z powrotem do oryginalnej postaci. Stochastyczność tego procesu poprawia jego siłę i zmusza niejawną przestrzeń do zapisywania wszędzie reprezentacji mających znaczenie: każdy punkt próbkowany w tej przestrzeni jest dekodowany do postaci poprawnego obiektu wejściowego."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![VAE](img\\8_4b.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Z technicznego punktu widzenia koder VAE działa w następujący sposób:\n",
    "\n",
    "1.\tModuł kodera zamienia próbki wejściowe input_img na dwa parametry niejawnej przestrzeni reprezentacji: z_mean i z_log_variance.\n",
    "2.\tPunkt z jest losowo próbkowany z niejawnego rozkładu normalnego, co ma doprowadzić do wygenerowania obrazu wejściowego za pomocą działania z = z_mean + exp(z_log_variance) * epsilon, gdzie epsilon jest losowym tensorem o małych wartościach.\n",
    "3.\tModuł dekodera mapuje ten punkt w niejawnej przestrzeni z powrotem na formę oryginalnego obrazu.\n",
    "\n",
    "Współczynnik epsilon przyjmuje wartość losową, a więc proces zapewnia to, że każdy punkt zbliżony w niejawnej przestrzeni do punktu, w którym zakodowano obraz input_img(z-mean) po rozkodowaniu będzie wyglądał podobnie do obrazu input_img (niejawna przestrzeń będzie miała charakter sensowny i ciągły). Po zdekodowaniu dowolnych dwóch zbliżonych do siebie punktów niejawnej przestrzeni uzyskamy bardzo podobne do siebie obrazy. Ciągłość w połączeniu z małą liczbą wymiarów niejawnej przestrzeni sprawia, że każdy kierunek tej przestrzeni tworzy oś zmienności danych o konkretnym znaczeniu — niejawna przestrzeń ma charakter wysoce ustrukturyzowany i łatwo poruszać się w niej przy użyciu wektorów koncepcji.\n",
    "\n",
    "Parametry kodera VAE są trenowane przy użyciu dwóch funkcji straty: straty rekonstrukcji, która wymusza dopasowanie dekodowanych próbek do obrazów wejściowych, i straty regularyzacji, która wspiera tworzenie niejawnych przestrzeni o poprawnej formie, a także umożliwia zmniejszenie nadmiernego dopasowania do treningowego zbioru danych. Przeanalizujmy szybko implementację kodera VAE w pakiecie Keras. Schematycznie wygląda ona tak:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'encoder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-27efa325e580>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Kodowanie obrazu wejściowego za pomocą parametrów średniej i wariancji.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mz_mean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mz_log_variance\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_img\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Wyciąganie punktu z przestrzeni przy użyciu małej losowej wartości epsilon.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mz_mean\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz_log_variance\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'encoder' is not defined"
     ]
    }
   ],
   "source": [
    "# Kodowanie obrazu wejściowego za pomocą parametrów średniej i wariancji.\n",
    "z_mean, z_log_variance = encoder(input_img)\n",
    "\n",
    "# Wyciąganie punktu z przestrzeni przy użyciu małej losowej wartości epsilon.\n",
    "z = z_mean + exp(z_log_variance) * epsilon\n",
    "\n",
    "# Dekodowanie — tworzenie obrazu.\n",
    "reconstructed_img = decoder(z)\n",
    "\n",
    "# Tworzenie instancji modelu autoenkodera mapującego obraz wejściowy na jego rekonstrukcję.\n",
    "model = Model(input_img, reconstructed_img)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model jest następnie trenowany z wykorzystaniem dwóch funkcji straty: straty rekonstrukcji i straty regularyzacji.\n",
    "Poniższy kod pokazuje sieć kodera mapującą obrazy na parametry rozkładu prawdopodobieństwa umieszczone w przestrzeni o niejawnym charakterze. W praktyce jest to prosta sieć konwolucyjna przypisująca obraz wejściowy x do dwóch wektorów: z_mean i z_log_var."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import layers\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "import numpy as np\n",
    "\n",
    "img_shape = (28, 28, 1)\n",
    "batch_size = 16\n",
    "latent_dim = 2  # Liczba wymiarów niejawnej przestrzeni: pracujemy z przestrzenią dwuwymiarową.\n",
    "\n",
    "input_img = keras.Input(shape=img_shape)\n",
    "\n",
    "x = layers.Conv2D(32, 3,\n",
    "                  padding='same', activation='relu')(input_img)\n",
    "x = layers.Conv2D(64, 3,\n",
    "                  padding='same', activation='relu',\n",
    "                  strides=(2, 2))(x)\n",
    "x = layers.Conv2D(64, 3,\n",
    "                  padding='same', activation='relu')(x)\n",
    "x = layers.Conv2D(64, 3,\n",
    "                  padding='same', activation='relu')(x)\n",
    "shape_before_flattening = K.int_shape(x)\n",
    "\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(32, activation='relu')(x)\n",
    "\n",
    "z_mean = layers.Dense(latent_dim)(x)\n",
    "z_log_var = layers.Dense(latent_dim)(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oto kod pozwalający na użycie parametrów rozkładu statystycznego z_mean i z_log_var, które z założenia mają pozwalać na utworzenie obrazu input_img w celu wygenerowania punktu z niejawnej przestrzeni. Część kodu zbudowanego na bazie zaplecza pakietu Keras obudowujemy warstwą Lambda. W Keras wszystko musi być warstwą, a więc kod, który nie należy do wbudowanej warstwy, powinien mieć formę warstwy Lambda lub innej samodzielnie zdefiniowanej warstwy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    epsilon = K.random_normal(shape=(K.shape(z_mean)[0], latent_dim),\n",
    "                              mean=0., stddev=1.)\n",
    "    return z_mean + K.exp(z_log_var) * epsilon\n",
    "\n",
    "z = layers.Lambda(sampling)([z_mean, z_log_var])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Poniższy fragment kodu przedstawia implementację dekodera. Wektor z jest modyfikowany tak, aby uzyskał wymiary obrazu, a następnie używanych jest kilka warstw konwolucyjnych w celu wygenerowania ostatecznej postaci obrazu wyjściowego mającego takie same wymiary jak oryginalny obraz input_img."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wejście wektora z.\n",
    "decoder_input = layers.Input(K.int_shape(z)[1:])\n",
    "\n",
    "# Zwiększanie rozdzielczości obiektu wejściowego.\n",
    "x = layers.Dense(np.prod(shape_before_flattening[1:]),\n",
    "                 activation='relu')(decoder_input)\n",
    "\n",
    "# Zmiana kształtu wektora w celu uzyskania map cech o takim samym kształcie jak kształt mapy cech przed ostatnią warstwą Flatten modułu kodującego.\n",
    "x = layers.Reshape(shape_before_flattening[1:])(x)\n",
    "\n",
    "# Warstwy Conv2DTranspose i Conv2D są używane w celu \n",
    "# odkodowania wektora z do formy mapy cech o takim samym rozmiarze\n",
    "# jak oryginalny obraz wejściowy.\n",
    "x = layers.Conv2DTranspose(32, 3,\n",
    "                           padding='same', activation='relu',\n",
    "                           strides=(2, 2))(x)\n",
    "x = layers.Conv2D(1, 3,\n",
    "                  padding='same', activation='sigmoid')(x)\n",
    "\n",
    "\n",
    "# Tworzenie instancji modelu dekodera zamieniajacego obiekt decoder_input na zdekodowany obraz.\n",
    "decoder = Model(decoder_input, x)\n",
    "\n",
    "# Przyjmuje wektor z i zwraca jego zdekodowaną postać.\n",
    "z_decoded = decoder(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dualizm funkcji straty kodera VAE nie wpasowuje się w tradycyjne ramy funkcji próbującej w formie loss(input, target). W związku z tym zdefiniujemy dodatkową warstwę, która będzie wewnętrznie korzystać z metody add_loss w celu wygenerowania wartości straty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomVariationalLayer(keras.layers.Layer):\n",
    "\n",
    "    def vae_loss(self, x, z_decoded):\n",
    "        x = K.flatten(x)\n",
    "        z_decoded = K.flatten(z_decoded)\n",
    "        xent_loss = keras.metrics.binary_crossentropy(x, z_decoded)\n",
    "        kl_loss = -5e-4 * K.mean(\n",
    "            1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "        return K.mean(xent_loss + kl_loss)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = inputs[0]\n",
    "        z_decoded = inputs[1]\n",
    "        loss = self.vae_loss(x, z_decoded)\n",
    "        self.add_loss(loss, inputs=inputs)\n",
    "        # Nie korzystamy z tych zwracanych danych, ale warstwa musi coś zwracać.\n",
    "        return x\n",
    "\n",
    "# Wywołujemy własną warstwę na obiekcie wejściowym i odkodowanym obiekcie wyjściowym \n",
    "# w celu wygenerowania ostatecznego obiektu generowanego przez model.\n",
    "y = CustomVariationalLayer()([input_img, z_decoded])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Teraz możemy utworzyć instancję modelu i ją wytrenować. Wartością straty zajęliśmy się w utworzonej ręcznie warstwie, a więc nie musimy określać zewnętrznej funkcji straty w czasie kompilacji (loss=None), co z kolei oznacza, że nie będziemy przekazywać docelowych danych w czasie trenowania (do funkcji fit modelu przekazujemy tylko argument x_train)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 28, 28, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 28, 28, 32)   320         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 14, 14, 64)   18496       conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 14, 14, 64)   36928       conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 14, 14, 64)   36928       conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 12544)        0           conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 32)           401440      flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 2)            66          dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 2)            66          dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 2)            0           dense_2[0][0]                    \n",
      "                                                                 dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "model_1 (Model)                 (None, 28, 28, 1)    56385       lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "custom_variational_layer_1 (Cus [(None, 28, 28, 1),  0           input_1[0][0]                    \n",
      "                                                                 model_1[1][0]                    \n",
      "==================================================================================================\n",
      "Total params: 550,629\n",
      "Trainable params: 550,629\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 289s 5ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 289s 5ms/step - loss: nan - val_loss: nan\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 309s 5ms/step - loss: nan - val_loss: nan\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 298s 5ms/step - loss: nan - val_loss: nan\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 328s 5ms/step - loss: nan - val_loss: nan\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 338s 6ms/step - loss: nan - val_loss: nan\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 321s 5ms/step - loss: nan - val_loss: nan\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 343s 6ms/step - loss: nan - val_loss: nan\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 353s 6ms/step - loss: nan - val_loss: nan\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 321s 5ms/step - loss: nan - val_loss: nan\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1878034d448>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "\n",
    "vae = Model(input_img, y)\n",
    "vae.compile(optimizer='rmsprop', loss=None)\n",
    "vae.summary()\n",
    "\n",
    "# Trenowanie kodera VAE na zbiorze cyfr MNIST.\n",
    "(x_train, _), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_train = x_train.reshape(x_train.shape + (1,))\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_test = x_test.reshape(x_test.shape + (1,))\n",
    "\n",
    "vae.fit(x=x_train, y=None,\n",
    "        shuffle=True,\n",
    "        epochs=10,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(x_test, None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Po wytrenowaniu modelu na zbiorze MNIST możemy użyć sieci decoder w celu wygenerowania obrazów na podstawie dowolnej niejawnej przestrzeni wektorów:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Wyświetlamy siatkę 15x15 cyfr (łącznie 255 cyfr).\n",
    "n = 15  \n",
    "digit_size = 28\n",
    "figure = np.zeros((digit_size * n, digit_size * n))\n",
    "# Transformacja liniowych współrzędnych przy użyciu funkcji ppf pakietu SciPy \n",
    "# w celu wygenerowania wartości niejawnej zmiennej z (pracujemy z przestrzenią Gaussa).\n",
    "grid_x = norm.ppf(np.linspace(0.05, 0.95, n))\n",
    "grid_y = norm.ppf(np.linspace(0.05, 0.95, n))\n",
    "\n",
    "for i, yi in enumerate(grid_x):\n",
    "    for j, xi in enumerate(grid_y):\n",
    "        z_sample = np.array([[xi, yi]])\n",
    "        z_sample = np.tile(z_sample, batch_size).reshape(batch_size, 2)\n",
    "        x_decoded = decoder.predict(z_sample, batch_size=batch_size)\n",
    "        digit = x_decoded[0].reshape(digit_size, digit_size)\n",
    "        figure[i * digit_size: (i + 1) * digit_size,\n",
    "               j * digit_size: (j + 1) * digit_size] = digit\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(figure)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wygenerowana siatka cyfr pokazuje całkowitą ciągłość rozkładu różnych klas cyfr — na kolejnych obrazach widać, że cyfry stopniowo przechodzą w siebie. Kierunki w tej przestrzeni mają znaczenie: podążając w jednym z wybranych kierunków, otrzymujemy obraz przypominający bardziej cyfrę 4, a podrażając w innym otrzymujemy obraz przypominający bardziej cyfrę 1 itd."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
